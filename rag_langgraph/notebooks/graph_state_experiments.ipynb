{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class MyState(MessagesState):\n",
    "    val: int\n",
    "    string: str\n",
    "    test: str\n",
    "\n",
    "def node_a(state: MyState):\n",
    "    return {\"val\": state[\"val\"]+1}\n",
    "\n",
    "def node_b(state: MyState):\n",
    "    return {\"string\": \"Hi\"}\n",
    "\n",
    "def node_c(state: MyState):\n",
    "    return {\"messages\": [HumanMessage(state[\"string\"])]}\n",
    "\n",
    "def node_d(state: MyState):\n",
    "    return {\"messages\": [AIMessage(\"Hello\")]}\n",
    "\n",
    "workflow = StateGraph(MyState)\n",
    "\n",
    "workflow.add_node(\"node_a\", node_a)\n",
    "workflow.add_node(\"node_b\", node_b)\n",
    "workflow.add_node(\"node_c\", node_c)\n",
    "workflow.add_node(\"node_d\", node_d)\n",
    "\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge(\"node_b\", \"node_c\")\n",
    "workflow.add_edge(\"node_c\", \"node_d\")\n",
    "workflow.add_edge(\"node_d\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23322c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"val\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_str = \"\"\n",
    "if temp_str:\n",
    "    print(1)\n",
    "else:\n",
    "    print(2)\n",
    "\n",
    "print(HumanMessage(temp_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[str], \"List of documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0aa0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(State.__annotations__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "LLM_ID = \"Qwen/Qwen3-8B\"\n",
    "OPENAI_URL = \"http://127.0.0.1:9805/v1\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=OPENAI_URL,\n",
    "    model=LLM_ID\n",
    ")\n",
    "\n",
    "result = llm.invoke([HumanMessage(\"Hwllo\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd7473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "class State(MessagesState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[str], \"List of documents\"]\n",
    "\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "OPENAI_URL = \"http://127.0.0.1:9804/v1\"\n",
    "MILVUS_URI = \"http://127.0.0.1:19530\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=OPENAI_URL,\n",
    "    model=EMBED_MODEL_ID\n",
    ")\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"doc_embeddings\",\n",
    "    connection_args={\n",
    "        \"uri\": MILVUS_URI,\n",
    "        \"token\": \"root:Milvus\",\n",
    "        \"db_name\": \"doc_embeddings\"\n",
    "    },\n",
    "    index_params={\n",
    "        \"index_type\": \"FLAT\",\n",
    "        \"metric_type\": \"L2\"\n",
    "    },\n",
    ")\n",
    "\n",
    "def retrieve(state: State):\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    documents = vector_store.similarity_search(question, k=3)\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a612b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retrieve({\"question\": \"스마트시티\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. \n",
    "Your primary mission is to answer questions based on provided context or chat history.\n",
    "Ensure your response is concise and directly addresses the question without any additional narration.\n",
    "\n",
    "###\n",
    "\n",
    "Your final answer should be written concisely (but include important numerical values, technical terms, jargon, and names), followed by the source of the information.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. Carefully read and understand the context provided.\n",
    "2. Identify the key information related to the question within the context.\n",
    "3. Formulate a concise answer based on the relevant information.\n",
    "4. Ensure your final answer directly addresses the question.\n",
    "5. List the source of the answer in bullet points, which must be a file name (with a page number) or URL from the context. Omit if the source cannot be found.\n",
    "\n",
    "# Output Format:\n",
    "[Your final answer here, with numerical values, technical terms, jargon, and names in their original language]\n",
    "\n",
    "**Source**(Optional)\n",
    "- (Source of the answer, must be a file name(with a page number) or URL from the context. Omit if you can't find the source of the answer.)\n",
    "- (list more if there are multiple sources)\n",
    "- ...\n",
    "\n",
    "###\n",
    "\n",
    "Remember:\n",
    "- It's crucial to base your answer solely on the **PROVIDED CONTEXT**. \n",
    "- DO NOT use any external knowledge or information not present in the given materials.\n",
    "- If you can't find the source of the answer, you should answer that you don't know.\n",
    "\n",
    "###\n",
    "\n",
    "# Here is the user's QUESTION that you should answer:\n",
    "{question}\n",
    "\n",
    "# Here is the CONTEXT that you should use to answer the question:\n",
    "{context}\n",
    "\n",
    "# Your final ANSWER to the user's QUESTION:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    formatted = []\n",
    "    for doc in docs:\n",
    "        page = doc.metadata.get(\"page\")\n",
    "        page_str = f\"<page>{page}</page>\" if isinstance(page, int) else \"\"\n",
    "        formatted.append(\n",
    "            f\"<document><content>{doc.page_content}</content>\"\n",
    "            f\"<source>{doc.metadata.get('source', 'unknown')}</source>\"\n",
    "            f\"{page_str}</document>\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54726c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = format_docs(result[\"documents\"][:1])\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = prompt | llm\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": format_docs(result[\"documents\"][:10]), \"question\": \"스마트 시티 어떻게 되가나?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a147412",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a685ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "a[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "type(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "File-Management-System-with-LLM (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
