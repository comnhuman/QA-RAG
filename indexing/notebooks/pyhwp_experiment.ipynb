{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"../huggingface_data\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "hwp_dir = Path(\"./hwp_files\")\n",
    "hwp_files = [\n",
    "    f for f in hwp_dir.rglob(\"*.hwp\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0024f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "from typing import Iterator\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "class HWPLoader(BSHTMLLoader):\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        self.original_file_path = file_path\n",
    "        \n",
    "        xml_text = subprocess.check_output(\n",
    "            [\"hwp5proc\", \"xml\", file_path],\n",
    "            text=True,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(\n",
    "            delete=False, suffix=\".xml\", mode=\"w\", encoding=\"utf-8\"\n",
    "        ) as tmp:\n",
    "            tmp.write(xml_text)\n",
    "            self._temp_path = tmp.name\n",
    "\n",
    "        super().__init__(file_path=self._temp_path, bs_kwargs={\"features\": \"xml\"})\n",
    "\n",
    "    def load(self):\n",
    "        docs = super().load()\n",
    "\n",
    "        try:\n",
    "            os.remove(self._temp_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        for doc in super().lazy_load():\n",
    "            doc.metadata[\"source\"] = f\"{Path(self.original_file_path)}\"\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for file_path in hwp_files:\n",
    "    try:\n",
    "        loader = HWPLoader(\n",
    "            file_path=str(file_path),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c58c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap = 10\n",
    ")\n",
    "\n",
    "# 4️⃣ 텍스트 분리\n",
    "texts = text_splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"../huggingface_data\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "from typing import Any, Iterable\n",
    "from openai import OpenAI\n",
    "from pymilvus import MilvusClient, DataType\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "# EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-4B\"\n",
    "OPENAI_URL = \"http://127.0.0.1:9804/v1\"\n",
    "MILVUS_URI = \"http://127.0.0.1:19530\"\n",
    "EXPORT_TYPE = \"doc_chunks\"\n",
    "CHUNKER = HybridChunker(tokenizer=EMBED_MODEL_ID, max_tokens=1000)\n",
    "\n",
    "all_files = [\"./hwp_files/Consulting of Asuncion Smart City for Digital District 20221130_최종.pptx\"]\n",
    "\n",
    "all_docs = []\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "        loader = DoclingLoader(\n",
    "            file_path=str(file_path),\n",
    "            export_type=EXPORT_TYPE,\n",
    "            chunker=CHUNKER,\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 1️⃣ 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-large-v2\")\n",
    "\n",
    "# 2️⃣ Text Splitter 정의\n",
    "splitter = TokenTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# 3️⃣ 테스트용 텍스트\n",
    "xml_text = \"<root>\" + \" \".join([\"hello world!\"] * 1000) + \"</root>\"\n",
    "\n",
    "# 4️⃣ Split 실행\n",
    "texts = splitter.split_text(xml_text)\n",
    "print(f\"{len(texts)} chunks created\")\n",
    "\n",
    "# 5️⃣ 검증 코드: 각 청크의 실제 토큰 개수를 계산\n",
    "for i, chunk in enumerate(texts[:3]):  # 앞의 몇 개만 확인\n",
    "    input_ids = tokenizer.encode(chunk, add_special_tokens=False)\n",
    "    print(f\"Chunk {i}: {len(input_ids)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=512,       # 모델 최대 토큰 길이에 맞게 조절 (예: 512, 1024 등)\n",
    ")\n",
    "\n",
    "# 4️⃣ 텍스트 분리\n",
    "texts = text_splitter.split_text(xml_text)\n",
    "print(len(texts), \"chunks created\")\n",
    "print(texts[0][:200])  # 첫 번째 청크 일부 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "doc = DocumentConverter().convert(source=DOC_SOURCE).document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"../huggingface_data\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "EXPORT_TYPE = \"doc_chunks\"\n",
    "CHUNKER = HybridChunker(tokenizer=EMBED_MODEL_ID, max_tokens=1000)\n",
    "\n",
    "# loader = DoclingLoader(\n",
    "#     file_path=str(_temp_path),\n",
    "#     export_type=EXPORT_TYPE,\n",
    "#     chunker=CHUNKER,\n",
    "# )\n",
    "\n",
    "# loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER.chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "class HWPLoader(BSHTMLLoader):\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        self.original_file_path = file_path\n",
    "        \n",
    "        xml_text = subprocess.check_output(\n",
    "            [\"hwp5proc\", \"xml\", file_path],\n",
    "            text=True,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(\n",
    "            delete=False, suffix=\".xml\", mode=\"w\", encoding=\"utf-8\"\n",
    "        ) as tmp:\n",
    "            tmp.write(xml_text)\n",
    "            self._temp_path = tmp.name\n",
    "\n",
    "        super().__init__(file_path=self._temp_path, bs_kwargs={\"features\": \"xml\"})\n",
    "\n",
    "    def load(self):\n",
    "        docs = super().load()\n",
    "\n",
    "        try:\n",
    "            os.remove(self._temp_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        for doc in super().lazy_load():\n",
    "            doc.metadata[\"source\"] = f\"{Path(self.original_file_path)}\"\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HWPLoader(hwp_files[0]).load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "File-Management-System-with-LLM (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
