{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import httpx\n",
    "from langchain_core.documents import Document\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[str], \"List of documents\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "RERANK_API_URL = \"http://127.0.0.1:9806/rerank\"\n",
    "RERANK_TIMEOUT_SEC = 15\n",
    "RERANK_TOP_K = 5\n",
    "RERANK_MIN_SCORE = 0.5\n",
    "RERANK_INSTRUCTION = \"Given a user query, retrieve relevant passages from the documents that answer the query.\"\n",
    "\n",
    "def _to_texts_and_keep(documents: list[Document]) -> tuple[list[str], list[Document]]:\n",
    "    texts: list[str] = []\n",
    "    kept: list[Document] = []\n",
    "    for doc in documents:\n",
    "        text = None\n",
    "        if hasattr(doc, \"page_content\"):\n",
    "            text = getattr(doc, \"page_content\", None)\n",
    "        else:\n",
    "            text = str(doc)\n",
    "\n",
    "        texts.append(text)\n",
    "        kept.append(doc)\n",
    "    return texts, kept\n",
    "\n",
    "\n",
    "def _post_rerank(question: str, texts: list[str]) -> list[float]:\n",
    "    payload = {\n",
    "        \"queries\": [question] * len(texts),\n",
    "        \"documents\": texts,\n",
    "        \"instruction\": RERANK_INSTRUCTION,\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Calling Rerank API: url={RERANK_API_URL}, n_docs={len(texts)}\")\n",
    "    # with httpx.Client(timeout=httpx.Timeout(RERANK_TIMEOUT_SEC)) as client:\n",
    "    with httpx.Client(timeout=None) as client:\n",
    "        resp = client.post(RERANK_API_URL, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "    scores = data.get(\"scores\", [])\n",
    "    if not isinstance(scores, list) or len(scores) != len(texts):\n",
    "        raise ValueError(f\"Invalid rerank response: {data}\")\n",
    "\n",
    "    return [float(s) for s in scores]\n",
    "\n",
    "\n",
    "def _apply_topk_and_threshold(order: list[int], scores: list[float]) -> list[int]:\n",
    "    if RERANK_MIN_SCORE is not None:\n",
    "        try:\n",
    "            thr = float(RERANK_MIN_SCORE)\n",
    "            order = [i for i in order if scores[i] >= thr]\n",
    "        except Exception:\n",
    "            logger.warning(\"RERANK_MIN_SCORE is not a float; ignoring threshold filter.\")\n",
    "\n",
    "    # TOP_K 제한\n",
    "    if RERANK_TOP_K is not None:\n",
    "        try:\n",
    "            k = RERANK_TOP_K\n",
    "            if k > 0:\n",
    "                order = order[:k]\n",
    "        except Exception:\n",
    "            logger.warning(\"RERANK_TOP_K is not an int; ignoring top-k.\")\n",
    "\n",
    "    return order\n",
    "\n",
    "\n",
    "def rerank(state: State):\n",
    "    question = state.get(\"question\")\n",
    "    documents = state.get(\"documents\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "\n",
    "    if not question:\n",
    "        logger.warning(\"rerank() called with empty question; returning state unchanged.\")\n",
    "        return\n",
    "\n",
    "    if not documents:\n",
    "        logger.info(\"rerank() no documents to rerank; returning state unchanged.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        texts, kept = _to_texts_and_keep(documents)\n",
    "        scores = _post_rerank(question, texts)\n",
    "\n",
    "        order = sorted(range(len(kept)), key=lambda i: scores[i], reverse=True)\n",
    "        order = _apply_topk_and_threshold(order, scores)\n",
    "\n",
    "        reranked_docs = [kept[i] for i in order]\n",
    "\n",
    "        # 로그: 상위 몇 개만 미리보기\n",
    "        preview_n = min(5, len(reranked_docs))\n",
    "        logger.info(\n",
    "            f\"Reranked {len(kept)} docs. \"\n",
    "            f\"Top-{preview_n} scores: {[round(scores[i], 4) for i in order[:preview_n]]}\"\n",
    "        )\n",
    "        return {\"documents\": reranked_docs}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed to rerank documents: {e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, time, threading\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[str], \"List of documents\"]\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "# EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-4B\"\n",
    "OPENAI_URL = \"http://127.0.0.1:9804/v1\"\n",
    "MILVUS_URI = \"http://127.0.0.1:19530\"\n",
    "TIMEOUT_SEC = 5\n",
    "\n",
    "def test_embedding_connection(embeddings: OpenAIEmbeddings, test_text: str, timeout: int) -> None:\n",
    "    result_container = {\"result\": None, \"error\": None}\n",
    "\n",
    "    def run():\n",
    "        try:\n",
    "            result_container[\"result\"] = embeddings.embed_query(test_text)\n",
    "        except Exception as e:\n",
    "            result_container[\"error\"] = e\n",
    "\n",
    "    thread = threading.Thread(target=run)\n",
    "    thread.start()\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        raise TimeoutError(f\"Embedding call exceeded timeout of {timeout} seconds.\")\n",
    "\n",
    "    if result_container[\"error\"]:\n",
    "        raise result_container[\"error\"]\n",
    "\n",
    "    result = result_container[\"result\"]\n",
    "    if result and isinstance(result, list) and len(result) > 0:\n",
    "        logger.info(f\"Embedding model connected successfully\")\n",
    "    else:\n",
    "        raise ValueError(\"Embedding response is invalid or empty.\")\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Initializing Embedding model: model='{EMBED_MODEL_ID}', url='{OPENAI_URL}'\")\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=OPENAI_URL,\n",
    "        model=EMBED_MODEL_ID,\n",
    "        tiktoken_enabled=False\n",
    "    )\n",
    "\n",
    "    test_embedding_connection(embeddings, \"연결 테스트 문장\", TIMEOUT_SEC)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed to initialize OpenAIEmbeddings: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Connecting to Milvus at {MILVUS_URI}\")\n",
    "    vector_store = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"doc_embeddings\",\n",
    "        connection_args={\n",
    "            \"uri\": MILVUS_URI,\n",
    "            \"token\": \"root:Milvus\",\n",
    "            \"db_name\": \"doc_embeddings3\"\n",
    "        },\n",
    "        index_params={\n",
    "            \"index_type\": \"FLAT\",\n",
    "            \"metric_type\": \"COSINE\"\n",
    "        },\n",
    "    )\n",
    "    logger.info(\"Milvus vector store connection established.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed to connect to Milvus: {e}\")\n",
    "    raise\n",
    "\n",
    "def retrieve(state: State):\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    human_message = HumanMessage(question)\n",
    "\n",
    "    if not question:\n",
    "        logger.warning(\"retrieve() called with empty question in state.\")\n",
    "        return {\"documents\": [], \"question\": \"\", \"messages\": [human_message]}\n",
    "\n",
    "    logger.info(f\"Retrieving documents for question: '{question}'\")\n",
    "\n",
    "    try:\n",
    "        print(question)\n",
    "        documents_with_score = vector_store.similarity_search_with_score(question, k=171)\n",
    "        documents = list(map(lambda x: x[0], documents_with_score))\n",
    "        logger.info(f\"Retrieved {len(documents)} documents from vector store.\")\n",
    "        return {\"documents\": documents, \"question\": question, \"messages\": [human_message], \"temp\": documents_with_score}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error during retrieval for question='{question}': {e}\")\n",
    "        return {\"documents\": [], \"question\": question, \"messages\": [human_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77106829",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state = {\n",
    "    \"question\": \"스마트 시티\"\n",
    "}\n",
    "\n",
    "my_state = retrieve(my_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state[\"temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state[\"temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbba2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state[\"temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in my_state[\"documents\"]:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in my_state[\"documents\"]:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in my_state[\"documents\"]:\n",
    "    print(doc.page_content[:100])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rerank(my_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f670a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "cli = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "cli.use_database(\"doc_embeddings3\")\n",
    "desc = cli.describe_collection(\"doc_embeddings\")\n",
    "# assert desc[\"schema\"][\"fields\"][-1][\"params\"][\"dim\"] == 4096, desc\n",
    "# assert desc[\"indexes\"][0][\"metric_type\"].upper() == \"COSINE\", desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "cli = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "cli.use_database(\"doc_embeddings3\")\n",
    "\n",
    "# 인덱스 목록\n",
    "idx_names = cli.list_indexes(\"doc_embeddings\")\n",
    "print(\"indexes:\", idx_names)  # 보통 ['vector']\n",
    "\n",
    "# 인덱스 상세 (메트릭/인덱스 타입 확인)\n",
    "idx_info = cli.describe_index(collection_name=\"doc_embeddings\", index_name=idx_names[0])\n",
    "print(idx_info)\n",
    "# 기대값: {'index_type': 'FLAT', 'metric_type': 'COSINE', ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "cli = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "cli.use_database(\"doc_embeddings2\")\n",
    "\n",
    "# 인덱스 목록\n",
    "idx_names = cli.list_indexes(\"doc_embeddings\")\n",
    "print(\"indexes:\", idx_names)  # 보통 ['vector']\n",
    "\n",
    "# 인덱스 상세 (메트릭/인덱스 타입 확인)\n",
    "idx_info = cli.describe_index(collection_name=\"doc_embeddings\", index_name=idx_names[0])\n",
    "print(idx_info)\n",
    "# 기대값: {'index_type': 'FLAT', 'metric_type': 'COSINE', ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5330b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "cli = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "cli.use_database(\"doc_embeddings2\")\n",
    "desc = cli.describe_collection(\"doc_embeddings\")\n",
    "\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "    embeddings = OpenAIEmbeddings(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=OPENAI_URL,\n",
    "        model=EMBED_MODEL_ID\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state[\"temp\"][-1][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8163f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state[\"temp\"][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "# pip install langchain openai numpy\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ OpenAI Embedding 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=OPENAI_URL,\n",
    "        model=EMBED_MODEL_ID\n",
    "    )\n",
    "\n",
    "# 2️⃣ 문장 정의\n",
    "sentence1 = my_state[\"temp\"][-1][0].page_content\n",
    "sentence2 = \"스마트 시티\"\n",
    "\n",
    "# 3️⃣ 문장 임베딩\n",
    "embedding1 = embeddings.embed_query(sentence1)\n",
    "embedding2 = embeddings.embed_query(sentence2)\n",
    "\n",
    "# 4️⃣ 코사인 유사도 계산 함수\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# 5️⃣ 결과 출력\n",
    "similarity = cosine_similarity(embedding1, embedding2)\n",
    "print(f\"코사인 유사도: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f111b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "# pip install langchain openai numpy\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ OpenAI Embedding 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=OPENAI_URL,\n",
    "        model=EMBED_MODEL_ID\n",
    "    )\n",
    "\n",
    "# 2️⃣ 문장 정의\n",
    "sentence1 = my_state[\"temp\"][-3][0].page_content\n",
    "sentence2 = \"스마트 시티\"\n",
    "\n",
    "# 3️⃣ 문장 임베딩\n",
    "embedding1 = embeddings.embed_query(sentence1)\n",
    "embedding2 = embeddings.embed_query(sentence2)\n",
    "\n",
    "# 4️⃣ 코사인 유사도 계산 함수\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# 5️⃣ 결과 출력\n",
    "similarity = cosine_similarity(embedding1, embedding2)\n",
    "print(f\"코사인 유사도: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f134f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state[\"temp\"][-3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 & 문서 텍스트\n",
    "query = \"스마트 시티\"\n",
    "doc_text = my_state[\"temp\"][1][0].page_content  # 같은 문서로 유지\n",
    "\n",
    "# 1) Milvus에서 받은 distance -> cosine similarity로 변환\n",
    "doc, dist = my_state[\"temp\"][1]   # (Document, distance)\n",
    "milvus_cos = 1.0 - float(dist)\n",
    "print(f\"[Milvus] 1 - distance = {milvus_cos:.6f}\")\n",
    "\n",
    "# 2) 같은 모델로, 같은 규칙으로 직접 코사인 계산\n",
    "q_vec = embeddings.embed_query(query)                 # 질의는 embed_query\n",
    "d_vec = embeddings.embed_documents([doc_text])[0]     # 문서는 embed_documents\n",
    "\n",
    "import numpy as np\n",
    "def cos_sim(a, b):\n",
    "    a = np.array(a); b = np.array(b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "manual_cos = cos_sim(q_vec, d_vec)\n",
    "print(f\"[Manual] cosine_similarity = {manual_cos:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63590059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "openai_client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=OPENAI_URL,\n",
    ")\n",
    "\n",
    "\n",
    "milvus_client = MilvusClient(\n",
    "    uri=MILVUS_URI,\n",
    "    token=\"root:Milvus\"\n",
    ")\n",
    "\n",
    "# 쿼리/결과 하나 집기\n",
    "query = \"스마트 시티\"\n",
    "docs_with_score = vector_store.similarity_search_with_relevance_scores(query, k=1)\n",
    "doc, dist = docs_with_score[0]   # dist = Milvus가 반환한 COSINE distance\n",
    "print(\"Milvus distance:\", dist)\n",
    "\n",
    "# 1) 같은 OpenAI-호환 클라이언트(인덱싱 때 쓰신 것)로 '쿼리' 임베딩 생성\n",
    "#    Qwen3는 'query'에 Instruction을 주는 걸 권장합니다 (모델카드 예시 참조)\n",
    "QWEN_QUERY_INSTRUCT = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: \"\n",
    "q_input = query\n",
    "# q_input = QWEN_QUERY_INSTRUCT + query\n",
    "q_vec = openai_client.embeddings.create(model=EMBED_MODEL_ID, input=[q_input]).data[0].embedding\n",
    "\n",
    "# 2) 방금 검색된 동일 문서의 'pk'를 확보(기본 primary_field는 'pk')\n",
    "doc_pk = doc.metadata.get(\"pk\") or doc.metadata.get(\"id\")\n",
    "\n",
    "\n",
    "milvus_client.use_database(\"doc_embeddings3\")\n",
    "# 3) Milvus에서 해당 문서의 '저장된 벡터'를 그대로 읽어오기\n",
    "row = milvus_client.query(\n",
    "    collection_name=\"doc_embeddings\",\n",
    "    filter=f\"pk == {int(doc_pk)}\" if doc_pk is not None else f'text == {json.dumps(doc.page_content)}',\n",
    "    output_fields=[\"pk\",\"text\",\"vector\"]\n",
    ")[0]\n",
    "stored_vec = row[\"vector\"]\n",
    "\n",
    "# 4) 코사인 유사도 직접 계산\n",
    "import numpy as np\n",
    "def cos(a,b):\n",
    "    a=np.array(a); b=np.array(b)\n",
    "    return float(np.dot(a,b) / (np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "\n",
    "manual_cos = cos(q_vec, stored_vec)\n",
    "print(\"Manual cosine:\", manual_cos)\n",
    "print(\"1 - distance :\", 1.0 - float(dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78197a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_client.query(\n",
    "    collection_name=\"doc_embeddings\",\n",
    "    filter=f\"pk == {int(doc_pk)}\" if doc_pk is not None else f'text == {json.dumps(doc.page_content)}',\n",
    "    output_fields=[\"pk\",\"text\",\"vector\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"openai>=1.52.0\" \"pymilvus>=2.4.3\" numpy\n",
    "\n",
    "import os, json, hashlib, logging\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# ---------------------------\n",
    "# 설정\n",
    "# ---------------------------\n",
    "OPENAI_URL = os.getenv(\"OPENAI_URL\", \"http://127.0.0.1:9804/v1\")  # OpenAI 호환 서버\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")\n",
    "EMBED_MODEL_ID  = os.getenv(\"EMBED_MODEL_ID\", \"Qwen/Qwen3-Embedding-8B\")\n",
    "\n",
    "MILVUS_URI  = os.getenv(\"MILVUS_URI\", \"http://127.0.0.1:19530\")\n",
    "DB_NAME     = os.getenv(\"MILVUS_DB\",   \"doc_embeddings7\")\n",
    "COLLECTION  = os.getenv(\"MILVUS_COL\",  \"doc_embeddings_plain\")\n",
    "\n",
    "# 전처리/프롬프트 일치 옵션\n",
    "STRIP_NEW_LINES = False  # LangChain을 쓰지 않으니, 여기서만 제어 (인덱싱/쿼리 둘 다 동일 적용)\n",
    "USE_QWEN_QUERY_INSTRUCT = True  # Qwen3-Embedding은 쿼리에 전용 지시문을 주는 것이 권장\n",
    "\n",
    "QWEN_QUERY_INSTRUCT = (\n",
    "    \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: \"\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s - %(message)s\")\n",
    "log = logging.getLogger(\"plain_milvus_demo\")\n",
    "\n",
    "# ---------------------------\n",
    "# 클라이언트\n",
    "# ---------------------------\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_URL)\n",
    "milvus_client = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "\n",
    "# ---------------------------\n",
    "# 전처리 & 임베딩 유틸\n",
    "# ---------------------------\n",
    "def normalize_text(t: str, strip_new_lines: bool = STRIP_NEW_LINES) -> str:\n",
    "    if t is None:\n",
    "        return \"\"\n",
    "    return t.replace(\"\\n\", \" \") if strip_new_lines else t\n",
    "\n",
    "def embed_documents(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"문서(패시지) 임베딩: 지시문 없이 그대로\"\"\"\n",
    "    inputs = [normalize_text(t) for t in texts]\n",
    "    resp = openai_client.embeddings.create(model=EMBED_MODEL_ID, input=inputs)\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def embed_query(text: str) -> List[float]:\n",
    "    \"\"\"쿼리 임베딩: Qwen3 계열은 쿼리용 지시문 권장\"\"\"\n",
    "    q = normalize_text(text)\n",
    "    # if USE_QWEN_QUERY_INSTRUCT:\n",
    "    #     q = QWEN_QUERY_INSTRUCT + q\n",
    "    resp = openai_client.embeddings.create(model=EMBED_MODEL_ID, input=[q])\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    va, vb = np.array(a, dtype=np.float32), np.array(b, dtype=np.float32)\n",
    "    denom = (np.linalg.norm(va) * np.linalg.norm(vb))\n",
    "    return float(np.dot(va, vb) / denom) if denom != 0 else 0.0\n",
    "\n",
    "# ---------------------------\n",
    "# Milvus 스키마/컬렉션 준비\n",
    "# ---------------------------\n",
    "def ensure_database(db_name: str):\n",
    "    if db_name not in milvus_client.list_databases():\n",
    "        milvus_client.create_database(db_name=db_name)\n",
    "    milvus_client.use_database(db_name)\n",
    "\n",
    "def ensure_collection(collection: str, dim: int):\n",
    "    if milvus_client.has_collection(collection):\n",
    "        # 이미 있다면 dim 확인은 생략 (테스트용). 실제 운영에선 dim 검증 권장.\n",
    "        return\n",
    "    schema = milvus_client.create_schema()\n",
    "    schema.add_field(\"pk\",   DataType.INT64,  is_primary=True, auto_id=False)\n",
    "    schema.add_field(\"text\", DataType.VARCHAR, max_length=65535)\n",
    "    schema.add_field(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "\n",
    "    index_params = milvus_client.prepare_index_params()\n",
    "    index_params.add_index(\n",
    "        field_name=\"vector\",\n",
    "        index_type=\"FLAT\",       # 재현성 위해 정확 검색\n",
    "        metric_type=\"COSINE\"     # COSINE 거리(작을수록 가깝다)\n",
    "    )\n",
    "    milvus_client.create_collection(\n",
    "        collection_name=collection,\n",
    "        schema=schema,\n",
    "        index_params=index_params\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# 데이터 적재\n",
    "# ---------------------------\n",
    "def make_pk(text: str) -> int:\n",
    "    return int(hashlib.sha256(text.encode(\"utf-8\")).hexdigest(), 16) % (1 << 63)\n",
    "\n",
    "def upsert_documents(docs: List[str]) -> None:\n",
    "    # 1) 임베딩\n",
    "    vectors = embed_documents(docs)\n",
    "    dim = len(vectors[0])\n",
    "    ensure_database(DB_NAME)\n",
    "    ensure_collection(COLLECTION, dim)\n",
    "\n",
    "    # 2) upsert\n",
    "    rows = []\n",
    "    for t, v in zip(docs, vectors):\n",
    "        rows.append({\"pk\": make_pk(t), \"text\": t, \"vector\": v})\n",
    "    milvus_client.upsert(collection_name=COLLECTION, data=rows)\n",
    "\n",
    "# ---------------------------\n",
    "# 서버측 검색(Milvus) & 수동 코사인 비교\n",
    "# ---------------------------\n",
    "def milvus_search(query: str, topk: int = 5) -> List[Tuple[dict, float]]:\n",
    "    qvec = embed_query(query)\n",
    "    res = milvus_client.search(\n",
    "        collection_name=COLLECTION,\n",
    "        data=[qvec],                   # 질의 벡터\n",
    "        anns_field=\"vector\",\n",
    "        limit=topk,\n",
    "        search_params={\"metric_type\": \"COSINE\"},\n",
    "        output_fields=[\"pk\",\"text\",\"vector\"]  # ← vector 불러오면 수동 코사인 비교도 즉시 가능\n",
    "    )\n",
    "    # res는 [hits] 형태\n",
    "    hits = res[0]\n",
    "    out = []\n",
    "    for h in hits:\n",
    "        # Milvus는 COSINE metric에서 \"distance\"를 반환 (작을수록 유사)\n",
    "        distance = float(h[\"distance\"])\n",
    "        # 코사인 유사도로 보려면:\n",
    "        cos_sim = 1.0 - distance\n",
    "        out.append((h, cos_sim))\n",
    "    return out\n",
    "\n",
    "def manual_rank(query: str, limit: int = 5) -> List[Tuple[dict, float]]:\n",
    "    qvec = embed_query(query)\n",
    "    # 데모를 위해 모든 행을 가져와 수동 계산 (데이터가 많다면 주의)\n",
    "    rows = milvus_client.query(\n",
    "        collection_name=COLLECTION,\n",
    "        filter=\"\",  # 전체\n",
    "        output_fields=[\"pk\",\"text\",\"vector\"],\n",
    "        limit=10000,              # ← 필수!\n",
    "        # offset=0               # 필요하면 페이징\n",
    "    )\n",
    "    scored = []\n",
    "    for r in rows:\n",
    "        sim = cosine_similarity(qvec, r[\"vector\"])\n",
    "        scored.append((r, sim))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:limit]\n",
    "\n",
    "# ---------------------------\n",
    "# 데모 실행\n",
    "# ---------------------------\n",
    "# 0) 예시 문장들(원하시는 문장으로 교체하세요)\n",
    "DOCS = [\n",
    "    \"스마트 시티는 도시의 인프라와 서비스를 데이터로 최적화한다.\",\n",
    "    \"스마트 팩토리는 공정 데이터를 활용해 생산성을 높인다.\",\n",
    "    \"서울은 교통 데이터 플랫폼을 통해 신호체계를 개선하고 있다.\",\n",
    "    \"도시 재생 프로젝트는 시민 참여와 공공 데이터를 결합한다.\",\n",
    "    \"클라우드 네이티브 아키텍처는 확장성과 복원력을 가진다.\"\n",
    "]\n",
    "QUERY = \"스마트 시티\"\n",
    "\n",
    "# 1) 문서 임베딩 + Milvus 적재\n",
    "log.info(\"문서 upsert\")\n",
    "upsert_documents(DOCS)\n",
    "\n",
    "# 2) Milvus 서버측 검색 (distance → cos로 변환)\n",
    "log.info(\"Milvus 검색 결과 (distance → 1 - distance 변환)\")\n",
    "milvus_hits = milvus_search(QUERY, topk=5)\n",
    "for i, (hit, cos_sim) in enumerate(milvus_hits, 1):\n",
    "    # hit에는 \"entity\" 필드가 있고, output_fields로 요청한 값들이 들어있다.\n",
    "    text = hit[\"entity\"].get(\"text\")\n",
    "    distance = float(hit[\"distance\"])\n",
    "    print(f\"[Milvus] top{i} | cos={cos_sim:.6f} | 1-dist={1-distance:.6f} | dist={distance:.6f} | text={text}\")\n",
    "\n",
    "# 3) 수동 코사인 랭킹 (저장된 vector 전부를 가져와서 쿼리 벡터와 직접 코사인)\n",
    "log.info(\"수동 코사인 랭킹\")\n",
    "manual_hits = manual_rank(QUERY, limit=5)\n",
    "for i, (row, cos_sim) in enumerate(manual_hits, 1):\n",
    "    print(f\"[Manual] top{i} | cos={cos_sim:.6f} | text={row['text']}\")\n",
    "\n",
    "# 4) 상호 검증: 같은 PK가 비슷한 순서로 나오는지 간단 확인\n",
    "print(\"\\n[검증] 상위 3개 텍스트 비교\")\n",
    "print(\"Milvus top3:\")\n",
    "print([h[0][\"entity\"][\"text\"] for h in milvus_hits[:3]])\n",
    "print(\"Manual top3:\")\n",
    "print([r[0][\"text\"] for r in manual_hits[:3]])\n",
    "\n",
    "print(\"\\n완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=OPENAI_URL,\n",
    "    model=EMBED_MODEL_ID\n",
    ")\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"doc_embeddings_plain\",\n",
    "    connection_args={\n",
    "        \"uri\": MILVUS_URI,\n",
    "        \"token\": \"root:Milvus\",\n",
    "        \"db_name\": \"doc_embeddings5\"\n",
    "    },\n",
    "    index_params={\n",
    "        \"index_type\": \"FLAT\",\n",
    "        \"metric_type\": \"COSINE\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, time, threading\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[str], \"List of documents\"]\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "# EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-4B\"\n",
    "OPENAI_URL = \"http://127.0.0.1:9804/v1\"\n",
    "MILVUS_URI = \"http://127.0.0.1:19530\"\n",
    "TIMEOUT_SEC = 5\n",
    "\n",
    "def test_embedding_connection(embeddings: OpenAIEmbeddings, test_text: str, timeout: int) -> None:\n",
    "    result_container = {\"result\": None, \"error\": None}\n",
    "\n",
    "    def run():\n",
    "        try:\n",
    "            result_container[\"result\"] = embeddings.embed_query(test_text)\n",
    "        except Exception as e:\n",
    "            result_container[\"error\"] = e\n",
    "\n",
    "    thread = threading.Thread(target=run)\n",
    "    thread.start()\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        raise TimeoutError(f\"Embedding call exceeded timeout of {timeout} seconds.\")\n",
    "\n",
    "    if result_container[\"error\"]:\n",
    "        raise result_container[\"error\"]\n",
    "\n",
    "    result = result_container[\"result\"]\n",
    "    if result and isinstance(result, list) and len(result) > 0:\n",
    "        logger.info(f\"Embedding model connected successfully\")\n",
    "    else:\n",
    "        raise ValueError(\"Embedding response is invalid or empty.\")\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Initializing Embedding model: model='{EMBED_MODEL_ID}', url='{OPENAI_URL}'\")\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=OPENAI_URL,\n",
    "        model=EMBED_MODEL_ID\n",
    "    )\n",
    "\n",
    "    test_embedding_connection(embeddings, \"연결 테스트 문장\", TIMEOUT_SEC)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed to initialize OpenAIEmbeddings: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Connecting to Milvus at {MILVUS_URI}\")\n",
    "    vector_store = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"doc_embeddings_plain\",\n",
    "        connection_args={\n",
    "            \"uri\": MILVUS_URI,\n",
    "            \"token\": \"root:Milvus\",\n",
    "            \"db_name\": \"doc_embeddings7\"\n",
    "        },\n",
    "        index_params={\n",
    "            \"index_type\": \"FLAT\",\n",
    "            \"metric_type\": \"COSINE\"\n",
    "        },\n",
    "    )\n",
    "    logger.info(\"Milvus vector store connection established.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed to connect to Milvus: {e}\")\n",
    "    raise\n",
    "\n",
    "def retrieve(state: State):\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    human_message = HumanMessage(question)\n",
    "\n",
    "    if not question:\n",
    "        logger.warning(\"retrieve() called with empty question in state.\")\n",
    "        return {\"documents\": [], \"question\": \"\", \"messages\": [human_message]}\n",
    "\n",
    "    logger.info(f\"Retrieving documents for question: '{question}'\")\n",
    "\n",
    "    try:\n",
    "        documents_with_score = vector_store.similarity_search_with_score(question, k=171)\n",
    "        documents = list(map(lambda x: x[0], documents_with_score))\n",
    "        logger.info(f\"Retrieved {len(documents)} documents from vector store.\")\n",
    "        return {\"documents\": documents, \"question\": question, \"messages\": [human_message], \"temp\": documents_with_score}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error during retrieval for question='{question}': {e}\")\n",
    "        return {\"documents\": [], \"question\": question, \"messages\": [human_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state = {\n",
    "    \"question\": \"스마트 시티\"\n",
    "}\n",
    "\n",
    "my_state = retrieve(my_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ec4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Manual] top1 | cos=0.669276 | text=스마트 시티는 도시의 인프라와 서비스를 데이터로 최적화한다.\n",
    "[Manual] top2 | cos=0.435555 | text=서울은 교통 데이터 플랫폼을 통해 신호체계를 개선하고 있다.\n",
    "[Manual] top3 | cos=0.385460 | text=스마트 팩토리는 공정 데이터를 활용해 생산성을 높인다.\n",
    "[Manual] top4 | cos=0.355128 | text=도시 재생 프로젝트는 시민 참여와 공공 데이터를 결합한다.\n",
    "[Manual] top5 | cos=0.217225 | text=클라우드 네이티브 아키텍처는 확장성과 복원력을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 & 문서 텍스트\n",
    "query = \"스마트 시티\"\n",
    "doc_text = my_state[\"temp\"][2][0].page_content  # 같은 문서로 유지\n",
    "\n",
    "# 1) Milvus에서 받은 distance -> cosine similarity로 변환\n",
    "doc, dist = my_state[\"temp\"][2]   # (Document, distance)\n",
    "milvus_cos =float(dist)\n",
    "print(f\"[Milvus] 1 - distance = {milvus_cos:.6f}\")\n",
    "\n",
    "# 2) 같은 모델로, 같은 규칙으로 직접 코사인 계산\n",
    "q_vec = embeddings.embed_query(query)                 # 질의는 embed_query\n",
    "d_vec = embeddings.embed_documents([doc_text])[0]     # 문서는 embed_documents\n",
    "\n",
    "import numpy as np\n",
    "def cos_sim(a, b):\n",
    "    a = np.array(a); b = np.array(b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "manual_cos = cos_sim(q_vec, d_vec)\n",
    "print(f\"[Manual] cosine_similarity = {manual_cos:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import json, numpy as np\n",
    "\n",
    "milvus_raw = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "milvus_raw.use_database(\"doc_embeddings5\")  # vector_store와 동일 DB\n",
    "\n",
    "# 1) Milvus 결과에서 문서 텍스트 확보\n",
    "doc_text = my_state[\"temp\"][0][0].page_content\n",
    "\n",
    "# 2) 저장 벡터 조회 (pk가 메타에 있으면 pk로, 없으면 text로)\n",
    "rows = milvus_raw.query(\n",
    "    collection_name=\"doc_embeddings_plain\",\n",
    "    filter=f'text == {json.dumps(doc_text)}',\n",
    "    output_fields=[\"pk\",\"text\",\"vector\"],\n",
    "    limit=1\n",
    ")\n",
    "stored_vec = rows[0][\"vector\"]\n",
    "\n",
    "# 3) 쿼리 벡터 (vector_store가 내부에서 쓴 것과 동일 경로로 생성)\n",
    "# q_vec = embeddings.embed_query(\"스마트 시티\")\n",
    "\n",
    "def cos(a,b):\n",
    "    a=np.array(a); b=np.array(b)\n",
    "    return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "\n",
    "manual_cos = cos(q_vec, stored_vec)\n",
    "print(f\"[Manual using STORED vec] cosine = {manual_cos:.6f}\")\n",
    "print(f\"[From Milvus] cosine = {float(my_state['temp'][0][1]):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Milvus] top1 | cos=0.330724 | 1-dist=0.330724 | dist=0.669276 | text=스마트 시티는 도시의 인프라와 서비스를 데이터로 최적화한다.\n",
    "[Milvus] top2 | cos=0.564445 | 1-dist=0.564445 | dist=0.435555 | text=서울은 교통 데이터 플랫폼을 통해 신호체계를 개선하고 있다.\n",
    "[Milvus] top3 | cos=0.614540 | 1-dist=0.614540 | dist=0.385460 | text=스마트 팩토리는 공정 데이터를 활용해 생산성을 높인다.\n",
    "[Milvus] top4 | cos=0.644872 | 1-dist=0.644872 | dist=0.355128 | text=도시 재생 프로젝트는 시민 참여와 공공 데이터를 결합한다.\n",
    "[Milvus] top5 | cos=0.782775 | 1-dist=0.782775 | dist=0.217225 | text=클라우드 네이티브 아키텍처는 확장성과 복원력을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee392d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe9f2c75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74bc126a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"openai>=1.52.0\" \"pymilvus>=2.4.3\" numpy\n",
    "\n",
    "import os, json, hashlib, logging\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# ---------------------------\n",
    "# 설정\n",
    "# ---------------------------\n",
    "OPENAI_URL = os.getenv(\"OPENAI_URL\", \"http://127.0.0.1:9804/v1\")  # OpenAI 호환 서버\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")\n",
    "EMBED_MODEL_ID  = os.getenv(\"EMBED_MODEL_ID\", \"Qwen/Qwen3-Embedding-8B\")\n",
    "\n",
    "MILVUS_URI  = os.getenv(\"MILVUS_URI\", \"http://127.0.0.1:19530\")\n",
    "DB_NAME     = os.getenv(\"MILVUS_DB\",   \"doc_embeddings102\")\n",
    "COLLECTION  = os.getenv(\"MILVUS_COL\",  \"doc_embeddings_plain\")\n",
    "\n",
    "# 전처리/프롬프트 일치 옵션\n",
    "STRIP_NEW_LINES = False  # LangChain을 쓰지 않으니, 여기서만 제어 (인덱싱/쿼리 둘 다 동일 적용)\n",
    "USE_QWEN_QUERY_INSTRUCT = True  # Qwen3-Embedding은 쿼리에 전용 지시문을 주는 것이 권장\n",
    "\n",
    "QWEN_QUERY_INSTRUCT = (\n",
    "    \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: \"\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s - %(message)s\")\n",
    "log = logging.getLogger(\"plain_milvus_demo\")\n",
    "\n",
    "# ---------------------------\n",
    "# 클라이언트\n",
    "# ---------------------------\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_URL)\n",
    "milvus_client = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "\n",
    "# ---------------------------\n",
    "# 전처리 & 임베딩 유틸\n",
    "# ---------------------------\n",
    "def normalize_text(t: str, strip_new_lines: bool = STRIP_NEW_LINES) -> str:\n",
    "    if t is None:\n",
    "        return \"\"\n",
    "    return t.replace(\"\\n\", \" \") if strip_new_lines else t\n",
    "\n",
    "def embed_documents(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"문서(패시지) 임베딩: 지시문 없이 그대로\"\"\"\n",
    "    inputs = [normalize_text(t) for t in texts]\n",
    "    resp = openai_client.embeddings.create(model=EMBED_MODEL_ID, input=inputs)\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def embed_query(text: str) -> List[float]:\n",
    "    \"\"\"쿼리 임베딩: Qwen3 계열은 쿼리용 지시문 권장\"\"\"\n",
    "    q = normalize_text(text)\n",
    "    # if USE_QWEN_QUERY_INSTRUCT:\n",
    "    #     q = QWEN_QUERY_INSTRUCT + q\n",
    "    resp = openai_client.embeddings.create(model=EMBED_MODEL_ID, input=[q])\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    va, vb = np.array(a, dtype=np.float32), np.array(b, dtype=np.float32)\n",
    "    denom = (np.linalg.norm(va) * np.linalg.norm(vb))\n",
    "    return float(np.dot(va, vb) / denom) if denom != 0 else 0.0\n",
    "\n",
    "# ---------------------------\n",
    "# Milvus 스키마/컬렉션 준비\n",
    "# ---------------------------\n",
    "def ensure_database(db_name: str):\n",
    "    if db_name not in milvus_client.list_databases():\n",
    "        milvus_client.create_database(db_name=db_name)\n",
    "    milvus_client.use_database(db_name)\n",
    "\n",
    "def ensure_collection(collection: str, dim: int):\n",
    "    if milvus_client.has_collection(collection):\n",
    "        # 이미 있다면 dim 확인은 생략 (테스트용). 실제 운영에선 dim 검증 권장.\n",
    "        return\n",
    "    schema = milvus_client.create_schema()\n",
    "    schema.add_field(\"pk\",   DataType.INT64,  is_primary=True, auto_id=False)\n",
    "    schema.add_field(\"text\", DataType.VARCHAR, max_length=65535)\n",
    "    schema.add_field(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "\n",
    "    index_params = milvus_client.prepare_index_params()\n",
    "    index_params.add_index(\n",
    "        field_name=\"vector\",\n",
    "        index_type=\"FLAT\",       # 재현성 위해 정확 검색\n",
    "        metric_type=\"COSINE\"     # COSINE 거리(작을수록 가깝다)\n",
    "    )\n",
    "    milvus_client.create_collection(\n",
    "        collection_name=collection,\n",
    "        schema=schema,\n",
    "        index_params=index_params\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# 데이터 적재\n",
    "# ---------------------------\n",
    "def make_pk(text: str) -> int:\n",
    "    return int(hashlib.sha256(text.encode(\"utf-8\")).hexdigest(), 16) % (1 << 63)\n",
    "\n",
    "def upsert_documents(docs: List[str]) -> None:\n",
    "    # 1) 임베딩\n",
    "    vectors = embed_documents(docs)\n",
    "    dim = len(vectors[0])\n",
    "    ensure_database(DB_NAME)\n",
    "    ensure_collection(COLLECTION, dim)\n",
    "\n",
    "    # 2) upsert\n",
    "    rows = []\n",
    "    for t, v in zip(docs, vectors):\n",
    "        rows.append({\"pk\": make_pk(t), \"text\": t, \"vector\": v})\n",
    "    milvus_client.upsert(collection_name=COLLECTION, data=rows)\n",
    "\n",
    "# ---------------------------\n",
    "# 서버측 검색(Milvus) & 수동 코사인 비교\n",
    "# ---------------------------\n",
    "def milvus_search(query: str, topk: int = 5) -> List[Tuple[dict, float]]:\n",
    "    qvec = embed_query(query)\n",
    "    res = milvus_client.search(\n",
    "        collection_name=COLLECTION,\n",
    "        data=[qvec],                   # 질의 벡터\n",
    "        anns_field=\"vector\",\n",
    "        limit=topk,\n",
    "        search_params={\"metric_type\": \"COSINE\"},\n",
    "        output_fields=[\"pk\",\"text\",\"vector\"]  # ← vector 불러오면 수동 코사인 비교도 즉시 가능\n",
    "    )\n",
    "    # res는 [hits] 형태\n",
    "    hits = res[0]\n",
    "    out = []\n",
    "    for h in hits:\n",
    "        # Milvus는 COSINE metric에서 \"distance\"를 반환 (작을수록 유사)\n",
    "        distance = float(h[\"distance\"])\n",
    "        # 코사인 유사도로 보려면:\n",
    "        cos_sim = 1.0 - distance\n",
    "        out.append((h, cos_sim))\n",
    "    return out\n",
    "\n",
    "def manual_rank(query: str, limit: int = 5) -> List[Tuple[dict, float]]:\n",
    "    qvec = embed_query(query)\n",
    "    # 데모를 위해 모든 행을 가져와 수동 계산 (데이터가 많다면 주의)\n",
    "    rows = milvus_client.query(\n",
    "        collection_name=COLLECTION,\n",
    "        filter=\"\",  # 전체\n",
    "        output_fields=[\"pk\",\"text\",\"vector\"],\n",
    "        limit=10000,              # ← 필수!\n",
    "        # offset=0               # 필요하면 페이징\n",
    "    )\n",
    "    scored = []\n",
    "    for r in rows:\n",
    "        sim = cosine_similarity(qvec, r[\"vector\"])\n",
    "        scored.append((r, sim))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:limit]\n",
    "\n",
    "# ---------------------------\n",
    "# 데모 실행\n",
    "# ---------------------------\n",
    "# 0) 예시 문장들(원하시는 문장으로 교체하세요)\n",
    "DOCS = [\n",
    "    \"스마트 시티는 도시의 인프라와 서비스를 데이터로 최적화한다.\",\n",
    "    \"스마트 팩토리는 공정 데이터를 활용해 생산성을 높인다.\",\n",
    "    \"서울은 교통 데이터 플랫폼을 통해 신호체계를 개선하고 있다.\",\n",
    "    \"도시 재생 프로젝트는 시민 참여와 공공 데이터를 결합한다.\",\n",
    "    \"클라우드 네이티브 아키텍처는 확장성과 복원력을 가진다.\"\n",
    "]\n",
    "QUERY = \"스마트 시티\"\n",
    "\n",
    "# 1) 문서 임베딩 + Milvus 적재\n",
    "log.info(\"문서 upsert\")\n",
    "upsert_documents(DOCS)\n",
    "\n",
    "# 2) Milvus 서버측 검색 (distance → cos로 변환)\n",
    "log.info(\"Milvus 검색 결과 (distance → 1 - distance 변환)\")\n",
    "milvus_hits = milvus_search(QUERY, topk=5)\n",
    "for i, (hit, cos_sim) in enumerate(milvus_hits, 1):\n",
    "    # hit에는 \"entity\" 필드가 있고, output_fields로 요청한 값들이 들어있다.\n",
    "    text = hit[\"entity\"].get(\"text\")\n",
    "    distance = float(hit[\"distance\"])\n",
    "    print(f\"[Milvus] top{i} | cos={distance:.6f} | 1-dist={1-cos_sim:.6f} | dist={cos_sim:.6f} | text={text}\")\n",
    "\n",
    "# 3) 수동 코사인 랭킹 (저장된 vector 전부를 가져와서 쿼리 벡터와 직접 코사인)\n",
    "log.info(\"수동 코사인 랭킹\")\n",
    "manual_hits = manual_rank(QUERY, limit=5)\n",
    "for i, (row, cos_sim) in enumerate(manual_hits, 1):\n",
    "    print(f\"[Manual] top{i} | cos={cos_sim:.6f} | text={row['text']}\")\n",
    "\n",
    "# 4) 상호 검증: 같은 PK가 비슷한 순서로 나오는지 간단 확인\n",
    "print(\"\\n[검증] 상위 3개 텍스트 비교\")\n",
    "print(\"Milvus top3:\")\n",
    "print([h[0][\"entity\"][\"text\"] for h in milvus_hits[:3]])\n",
    "print(\"Manual top3:\")\n",
    "print([r[0][\"text\"] for r in manual_hits[:3]])\n",
    "\n",
    "print(\"\\n완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eba4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, time, threading\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[str], \"List of documents\"]\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "# EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-4B\"\n",
    "OPENAI_URL = \"http://127.0.0.1:9804/v1\"\n",
    "MILVUS_URI = \"http://127.0.0.1:19530\"\n",
    "TIMEOUT_SEC = 5\n",
    "\n",
    "def test_embedding_connection(embeddings: OpenAIEmbeddings, test_text: str, timeout: int) -> None:\n",
    "    result_container = {\"result\": None, \"error\": None}\n",
    "\n",
    "    def run():\n",
    "        try:\n",
    "            result_container[\"result\"] = embeddings.embed_query(test_text)\n",
    "        except Exception as e:\n",
    "            result_container[\"error\"] = e\n",
    "\n",
    "    thread = threading.Thread(target=run)\n",
    "    thread.start()\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        raise TimeoutError(f\"Embedding call exceeded timeout of {timeout} seconds.\")\n",
    "\n",
    "    if result_container[\"error\"]:\n",
    "        raise result_container[\"error\"]\n",
    "\n",
    "    result = result_container[\"result\"]\n",
    "    if result and isinstance(result, list) and len(result) > 0:\n",
    "        logger.info(f\"Embedding model connected successfully\")\n",
    "    else:\n",
    "        raise ValueError(\"Embedding response is invalid or empty.\")\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Initializing Embedding model: model='{EMBED_MODEL_ID}', url='{OPENAI_URL}'\")\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=OPENAI_URL,\n",
    "        model=EMBED_MODEL_ID\n",
    "    )\n",
    "\n",
    "    test_embedding_connection(embeddings, \"연결 테스트 문장\", TIMEOUT_SEC)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed to initialize OpenAIEmbeddings: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Connecting to Milvus at {MILVUS_URI}\")\n",
    "    vector_store = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"doc_embeddings_plain\",\n",
    "        connection_args={\n",
    "            \"uri\": MILVUS_URI,\n",
    "            \"token\": \"root:Milvus\",\n",
    "            \"db_name\": \"doc_embeddings102\"\n",
    "        },\n",
    "        index_params={\n",
    "            \"index_type\": \"FLAT\",\n",
    "            \"metric_type\": \"COSINE\"\n",
    "        },\n",
    "    )\n",
    "    logger.info(\"Milvus vector store connection established.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed to connect to Milvus: {e}\")\n",
    "    raise\n",
    "\n",
    "def retrieve(state: State):\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    human_message = HumanMessage(question)\n",
    "\n",
    "    if not question:\n",
    "        logger.warning(\"retrieve() called with empty question in state.\")\n",
    "        return {\"documents\": [], \"question\": \"\", \"messages\": [human_message]}\n",
    "\n",
    "    logger.info(f\"Retrieving documents for question: '{question}'\")\n",
    "\n",
    "    try:\n",
    "        documents_with_score = vector_store.similarity_search_with_score(question, k=171)\n",
    "        documents = list(map(lambda x: x[0], documents_with_score))\n",
    "        logger.info(f\"Retrieved {len(documents)} documents from vector store.\")\n",
    "        return {\"documents\": documents, \"question\": question, \"messages\": [human_message], \"temp\": documents_with_score}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error during retrieval for question='{question}': {e}\")\n",
    "        return {\"documents\": [], \"question\": question, \"messages\": [human_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state = {\n",
    "    \"question\": \"스마트 시티\"\n",
    "}\n",
    "\n",
    "my_state = retrieve(my_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf736ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import json, numpy as np\n",
    "\n",
    "milvus_raw = MilvusClient(uri=MILVUS_URI, token=\"root:Milvus\")\n",
    "milvus_raw.use_database(\"doc_embeddings5\")  # vector_store와 동일 DB\n",
    "\n",
    "# 1) Milvus 결과에서 문서 텍스트 확보\n",
    "doc_text = my_state[\"temp\"][-1][0].page_content\n",
    "\n",
    "# 2) 저장 벡터 조회 (pk가 메타에 있으면 pk로, 없으면 text로)\n",
    "rows = milvus_raw.query(\n",
    "    collection_name=\"doc_embeddings_plain\",\n",
    "    filter=f'text == {json.dumps(doc_text)}',\n",
    "    output_fields=[\"pk\",\"text\",\"vector\"],\n",
    "    limit=1\n",
    ")\n",
    "stored_vec = rows[0][\"vector\"]\n",
    "\n",
    "# 3) 쿼리 벡터 (vector_store가 내부에서 쓴 것과 동일 경로로 생성)\n",
    "q_vec = embeddings.embed_query(\"스마트 시티\")\n",
    "\n",
    "def cos(a,b):\n",
    "    a=np.array(a); b=np.array(b)\n",
    "    return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "\n",
    "manual_cos = cos(q_vec, stored_vec)\n",
    "print(f\"[Manual using STORED vec] cosine = {manual_cos:.6f}\")\n",
    "print(f\"[From Milvus] cosine = {float(my_state['temp'][-1][1]):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff51d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = openai_client.embeddings.create(model=EMBED_MODEL_ID, input=[\"스마트 시티\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c53f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf010749",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos(a.data[0].embedding, q_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc603c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "emb = OpenAIEmbeddings(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=OPENAI_URL,\n",
    "        model=EMBED_MODEL_ID\n",
    "    )\n",
    "v1 = emb.embed_query(\"스마트 시티\")\n",
    "\n",
    "# OpenAI SDK (공식)\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_URL)\n",
    "v2 = client.embeddings.create(model=EMBED_MODEL_ID, input=[\"스마트 시티\"]).data[0].embedding\n",
    "\n",
    "cos(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "OPENAI_URL = \"http://127.0.0.1:9804/v1\"\n",
    "OPENAI_API_KEY = \"EMPTY\"\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "\n",
    "# LangChain — 키/엔드포인트/차원/encoding_format 통일\n",
    "emb = OpenAIEmbeddings(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=OPENAI_URL,\n",
    "    model=EMBED_MODEL_ID,\n",
    "    model_kwargs={\"encoding_format\": \"float\"},\n",
    "    # tiktoken_enabled=False,      # 호환 서버 사용 시 권장\n",
    ")\n",
    "\n",
    "v1 = emb.embed_query(\"스마트 시티\")                  # 문자열\n",
    "v1b = emb.embed_documents([\"스마트 시티\"])[0]        # 리스트 형태와도 비교\n",
    "\n",
    "# OpenAI SDK — 입력도 문자열로 맞춰 보기\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_URL)\n",
    "v2 = client.embeddings.create(\n",
    "    model=EMBED_MODEL_ID, input=\"스마트 시티\"\n",
    ").data[0].embedding\n",
    "\n",
    "def cos(a,b):\n",
    "    a=np.array(a); b=np.array(b)\n",
    "    return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "\n",
    "print(\"LC(query) vs SDK(str):\", cos(v1, v2))\n",
    "print(\"LC(doc)   vs SDK(str):\", cos(v1b, v2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "File-Management-System-with-LLM (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
